# -*- coding: utf-8 -*-
"""437_g6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IBlt43pjAjgLttqxM84W6JP9XxW7UB22
"""

import missingno as msno
import matplotlib
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt

from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,VotingClassifier

from sklearn.metrics import *
from sklearn.model_selection import *
import numpy as np


from imblearn.over_sampling import SMOTE

"""READ DATABASE"""

database=pd.read_csv('/content/dataset.csv')

"""# DATA VISUALIZATION"""

print(type(database))

print(database.shape)

database.head()

##import missingno as msno
A= '#7DA5C9'

color = [A, A, A, A, A, A, A, A, A, A,'#4B6378','#4B6378']
fig, ax = plt.subplots(figsize=(10, 2), dpi=80)
fig.patch.set_facecolor('#ffffff')
ax.set_facecolor('#ffffff')
msno.bar(database, sort='descending', color=color, ax=ax, fontsize=8, labels='off', filter='top')
ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='center', size=10, weight='normal',  alpha=1)
ax.set_yticklabels('')
ax.spines['bottom'].set_visible(True)
plt.show()

database.isnull().sum()

database.info()

database[['bmi']]

"""# Imputing in BMI"""

impute = SimpleImputer(missing_values=np.nan, strategy='mean')

impute.fit(database[['bmi']])

database['bmi'] = impute.transform(database[['bmi']])

database.isnull().sum()

"""# removing ID coloum

"""

database=database.drop(['id'],axis=1)

database.info()

database.shape

database.describe()

database.duplicated().sum()

database.count()

"""# CARAGORICAL FEATURES

"""

category_cols = database.select_dtypes(include=['object'])
sns.set_palette('magma')
for col in category_cols:
    sns.countplot(data=database, x=col, order=database[col].value_counts().index)
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.show()

"""# Feature Engineering"""

database['work_type'].unique()

database['work_type'].value_counts()

database = pd.get_dummies(database,columns=["work_type"],dtype=int)  #binary te ney

database.head()

database['gender'].unique()

database["gender"]=database["gender"].map({"Male":1,"Female":0})

database['gender'].head()

database.count()

database['gender'].mean()

database["gender"]=database["gender"].fillna(database["gender"].mean())

#database["gender"].fillna(database["gender"].mean(), inplace=True)

database['ever_married'].unique()

database["ever_married"]=database["ever_married"].map({"Yes":1,"No":0})

database['Residence_type'].unique()

database["Residence_type"]=database["Residence_type"].map({"Urban":1,"Rural":0})

database['smoking_status'].unique()

database = pd.get_dummies(database,columns=["smoking_status"],dtype=int)

database.head()

database.count()

"""### SELECTION

"""

import seaborn as sns
correlation=database.corr()
correlation

sns.heatmap(correlation, cmap = 'YlGnBu')

"""# Spliting

"""

print("Keys of dataset:\n", database.keys())

y=database["stroke"]
X=database.drop(["stroke"],axis=1)

X.keys()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state=0, stratify = y)

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)

print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

import pandas as pd
stroke_dataframe = pd.DataFrame(X_train, columns=X.keys())

import matplotlib.pyplot as plt
# for col in stroke_dataframe.select_dtypes(include=['bool']).columns:
#     stroke_dataframe[col] = stroke_dataframe[col].astype(int)
scatter_matrix = pd.plotting.scatter_matrix(stroke_dataframe, c=y_train, figsize=(50, 30),
                                           marker='o', hist_kwds={'bins': 20}, s=60,
                                           alpha=.5)


plt.show()

fig,ax = plt.subplots()
ax.hist(y_train)
ax.set_title('frequency of  classes')
ax.set_xlabel('class label')
ax.set_ylabel('no of strokes')

"""# SCALING

"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

scaler.fit(X_train)

X_train_scaled = scaler.transform(X_train)

print("per-feature minimum before scaling:\n {}".format(X_train.min(axis=0)))
print("per-feature maximum before scaling:\n {}".format(X_train.max(axis=0)))

print("per-feature minimum after scaling:\n {}".format(
    X_train_scaled.min(axis=0)))
print("per-feature maximum after scaling:\n {}".format(
    X_train_scaled.max(axis=0)))

X_test_scaled = scaler.transform(X_test)

print("per-feature minimum before scaling:\n {}".format(X_test.min(axis=0)))
print("per-feature maximum before scaling:\n {}".format(X_test.max(axis=0)))

print("per-feature minimum after scaling:\n {}".format(
    X_test_scaled.min(axis=0)))
print("per-feature maximum after scaling:\n {}".format(
    X_test_scaled.max(axis=0)))



"""#EFFECT OF USING MinMax Scalar

min max_feature scaling
"""

from sklearn.neighbors import KNeighborsClassifier

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state=0, stratify = y)

knn=KNeighborsClassifier()

knn.fit(X_train, y_train)
scaler = MinMaxScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)

X_test_scaled = scaler.transform(X_test)
#train
knn.fit(X_train_scaled, y_train)
print("Scaled test set accuracy: {:.2f}".format(knn.score(X_test_scaled, y_test)))

"""#cross validation"""

from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import *



# Decision Tree
model2 = DecisionTreeClassifier()
dt_accuracy = cross_val_score(model2, X, y, cv=10, scoring='accuracy').mean()
print("Decision Tree Accuracy:", dt_accuracy)

# Random Forest
model4 = RandomForestClassifier()
rf_accuracy = cross_val_score(model4, X, y, cv=10, scoring='accuracy').mean()
print("Random Forest Accuracy:", rf_accuracy)

# Gradient Boosting
model3 = GradientBoostingClassifier()
gb_accuracy = cross_val_score(model3, X, y, cv=10, scoring='accuracy').mean()
print("Gradient Boosting Accuracy:", gb_accuracy)

# K-Nearest Neighbors
model5 = KNeighborsClassifier()
knn_accuracy = cross_val_score(model5, X, y, cv=10, scoring='accuracy').mean()
print("K-Nearest Neighbors Accuracy:", knn_accuracy)

# Logistic Regression
model1 = LogisticRegression(solver="liblinear")
lr_accuracy = cross_val_score(model1, X, y, cv=10, scoring='accuracy').mean()
print("Logistic Regression Accuracy:", lr_accuracy)



# Voting Classifier
model6 = VotingClassifier(estimators=[("lr", model1), ("dtf", model2), ("rf", model4),
                                      ("gb", model3), ("knn", model5)], voting="soft")
vc_accuracy = cross_val_score(model6, X, y, cv=10, scoring='accuracy').mean()
print("Voting Classifier Accuracy:", vc_accuracy)

"""**SMOTE for Imbalanced Classification**   not sure how it works"""

from imblearn.over_sampling import SMOTE
y=database["stroke"]
X=database.drop(["stroke"],axis=1)
smote=SMOTE()
X_train,y_train=smote.fit_resample(X_train,y_train)
num_stroke_before = y.sum()
num_stroke_after = y_train.sum()

plt.bar(['Before', 'After'], [num_stroke_before, num_stroke_after])
plt.title('Number of Stroke Cases Before and After Oversampling')
plt.xlabel('Oversampling')
plt.ylabel('Number of Stroke Cases')
plt.show()

from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris

"""###################################

# **Logistic regression**
"""

from sklearn.metrics import accuracy_score
model_1=LogisticRegression(solver='liblinear')
model_1.fit(X_train,y_train)
y_hat=model_1.predict(X_test)
v = accuracy_score(y_test,y_hat)
val1= v
print('Logistic Regression :',v*100,"%")

print(classification_report(y_test,y_hat))

predictions = model_1.predict(X_test)
from sklearn.metrics import confusion_matrix
mat=confusion_matrix(predictions, y_test)
print(mat)
from seaborn import heatmap


plt.figure(figsize=(8, 6))
sns.heatmap(mat, cmap="Pastel1_r", xticklabels=['class_0', 'class_1'], yticklabels=['class_0', 'class_1'], annot=True, fmt="d")

plt.show()

"""# RandomForestClassifier"""

model_4= RandomForestClassifier()
model_4.fit(X_train,y_train)
y_hat=model_4.predict(X_test)
v = accuracy_score(y_test,y_hat)
val4 = v
print('RandomForestClassifier:',v*100,"%")

print(classification_report(y_test,y_hat))

predictions = model_4.predict(X_test)
from sklearn.metrics import confusion_matrix
mat=confusion_matrix(predictions, y_test)
print(mat)
from seaborn import heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(mat , cmap="Pastel1_r", xticklabels=['class_0' ,'class_1'], yticklabels=['class_0' ,'class_1'], annot=True,fmt="d")
plt.show()

"""# **KnnClassifier**"""

model_3= KNeighborsClassifier(n_neighbors = 5)
model_3.fit(X_train,y_train)
y_hat=model_3.predict(X_test)

v = accuracy_score(y_test,y_hat)
val2=v
print('KNN :',v*100,"%")

print(classification_report(y_test,y_hat))

predictions = model_3.predict(X_test)
from sklearn.metrics import confusion_matrix
mat=confusion_matrix(predictions, y_test)
print(mat)
from seaborn import heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(mat, cmap="Pastel1_r", xticklabels=['class_0', 'class_1'], yticklabels=['class_0', 'class_1'], annot=True, fmt="d")

plt.show()

"""# Gradint Boosting"""

model_2= GradientBoostingClassifier()
model_2.fit(X_train,y_train)
y_hat=model_2.predict(X_test)

v = accuracy_score(y_test,y_hat)
val5 =v
print('GradientBoosting :',v*100,"%")

print(classification_report(y_test,y_hat))

predictions = model_2.predict(X_test)
from sklearn.metrics import confusion_matrix
mat=confusion_matrix(predictions, y_test)
print(mat)
from seaborn import heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(mat, cmap="Pastel1_r", xticklabels=['class_0', 'class_1'], yticklabels=['class_0', 'class_1'], annot=True, fmt="d")

plt.show()

"""# DecisionTree"""

model_5= DecisionTreeClassifier()
model_5.fit(X_train,y_train)
y_hat=model_5.predict(X_test)


v = accuracy_score(y_test,y_hat)
val3=v
print('KNN :',v*100,"%")

print(classification_report(y_test,y_hat))

predictions = model_5.predict(X_test)
from sklearn.metrics import confusion_matrix
mat=confusion_matrix(predictions, y_test)
print(mat)
from seaborn import heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(mat, cmap="Pastel1_r", xticklabels=['class_0', 'class_1'], yticklabels=['class_0', 'class_1'], annot=True, fmt="d")

plt.show()

"""# Graphs

"""

RocCurveDisplay.from_estimator(model_1,X_test,y_test)
RocCurveDisplay.from_estimator(model_5,X_test,y_test)
RocCurveDisplay.from_estimator(model_3,X_test,y_test)
RocCurveDisplay.from_estimator(model_4,X_test,y_test)
RocCurveDisplay.from_estimator(model_2,X_test,y_test)

"""#CHECK ERROR  QUALITY"""

models = {
    "Logistic Regression": val1*100,
    "K-Nearest Neighbors": val2*100,
    "Decision Tree": val3*100,
    "Random Forest": val4*100,
    "Gradient Boosting": val5*100

}
print(val1,val2,val3,val4,val5)
name = list(models.keys())
accu = list(models.values())
error = []
for i in list(models.values()):
  print("this is i ",i)
  print("this is error ",100-i)
  error.append(100-i)

plt.bar(name, accu, label='Accuracy', color='#96b6af', width = 0.4)
plt.xlabel('Model')
plt.ylabel('Percentage')
plt.title('Model Accuracy')
plt.xticks(rotation=90)
plt.legend()
plt.show()

"""# E R R O R"""

plt.bar(name, error , label='Error', color='pink')
plt.xlabel('Model')
plt.ylabel('Percentage')
plt.title('Error')
plt.xticks(rotation=90)
plt.legend()
plt.show()